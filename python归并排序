import torch
import torch_npu
import torchair
import os
import torch.distributed._functional_collectives as funcol
from typing import Any, Dict, Iterator, List, Optional, Tuple, Union
import torch.distributed as dist
from torch.distributed import distributed_c10d
import torch.distributed.distributed_c10d as c10d
from torch._decomp import register_decomposition
# import torchair.ge_concrete_graph.ge_converter.experimental.patch_for_hcom_allreduce
import torchair.ge_concrete_graph.ge_converter.experimental.hcom_alltoall
from torch._decomp import get_decompositions


class All2allsinge(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, input, output):
        dist.all_to_all_single(output, input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1])
        # dist.all_to_all_single(output, input)
        return output

def test_alltoall_single_dynamic(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input = torch.arange(4) + rank * 4
    input = input.npu()
    print("input: ", input)
    output = output = torch.empty([4], dtype=torch.int64).npu()

    model = All2allsinge().npu()

    from torchair.core.utils import logger
    import logging
    from torchair.configs.compiler_config import CompilerConfig
    config = CompilerConfig()
    # logger.setLevel(logging.DEBUG)
    config.debug.graph_dump.type = "pbtxt"
    # config.experimental_config.static_model_ops_lower_limit = 4
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    dist.all_to_all_single(output, input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1])
    model = torch.compile(model, backend=npu_backend, dynamic=True, fullgraph=True)
    # dist.all_to_all_single(output, input, output_split_sizes=[3, 1], input_split_sizes=[1, 3])
    with torch.no_grad():
        output = model(input, output)

    # output = all_to_all_single(input, output_split_sizes=[2, 2], input_split_sizes=[2, 2], group=distributed_c10d._get_default_group()) # 会报错。也不提示是size给错了，这个有问题
    # output = all_to_all_single(input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1], group=distributed_c10d._get_default_group())
    # output = funcol.all_to_all_single(input, output_split_sizes=[1, 3], input_split_sizes=[1, 3], group=distributed_c10d._get_default_group())
    print("output: ", output)
    dist.destroy_process_group()


class All2allsinge_split_size(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, input, output, input_split_sizes, output_split_sizes):
        dist.all_to_all_single(output, input, output_split_sizes=output_split_sizes, input_split_sizes=input_split_sizes)
        return output


def test_alltoall_single_dynamic_split_size(rank, world_size):

    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    if rank == 0:
        input = torch.tensor([0, 1, 2, 3, 4, 5], dtype=torch.int64).npu()
        output = torch.empty([9], dtype=torch.int64).npu()
        input_split_sizes = [2, 2, 1, 1]
        output_split_sizes = [2, 3, 2, 2]
    elif rank == 1:
        input = torch.tensor([10, 11, 12, 13, 14, 15, 16, 17, 18], dtype=torch.int64).npu()
        output = torch.empty([7], dtype=torch.int64).npu()
        input_split_sizes = [3, 2, 2, 2]
        output_split_sizes = [2, 2, 1, 2]
    elif rank == 2:
        input = torch.tensor([20, 21, 22, 23, 24], dtype=torch.int64).npu()
        output = torch.empty([6], dtype=torch.int64).npu()
        input_split_sizes = [2, 1, 1, 1]  
        output_split_sizes = [1, 2, 1, 2]
    elif rank == 3:
        input = torch.tensor([30, 31, 32, 33, 34, 35, 36], dtype=torch.int64).npu()
        output = torch.empty([5], dtype=torch.int64).npu()
        input_split_sizes = [2, 2, 2, 1]
        output_split_sizes = [1, 2, 1, 1]

    print("input: ", input)

    model = All2allsinge_split_size().npu()

    from torchair.core.utils import logger
    import logging
    from torchair.configs.compiler_config import CompilerConfig
    config = CompilerConfig()
    # logger.setLevel(logging.DEBUG)
    config.debug.graph_dump.type = "pbtxt"
    # config.experimental_config.static_model_ops_lower_limit = 4
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    # dist.all_to_all_single(output, input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1])
    model = torch.compile(model, backend=npu_backend, dynamic=True, fullgraph=True)
    # dist.all_to_all_single(output, input, output_split_sizes=[3, 1], input_split_sizes=[1, 3])
    with torch.no_grad():
        output = model(input, output, input_split_sizes, output_split_sizes)

    # output = all_to_all_single(input, output_split_sizes=[2, 2], input_split_sizes=[2, 2], group=distributed_c10d._get_default_group()) # 会报错。也不提示是size给错了，这个有问题
    # output = all_to_all_single(input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1], group=distributed_c10d._get_default_group())
    # output = funcol.all_to_all_single(input, output_split_sizes=[1, 3], input_split_sizes=[1, 3], group=distributed_c10d._get_default_group())
    print("output: ", output)
    dist.destroy_process_group()

def test_alltoall_single_static_split_size(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    if rank == 0:
        input = torch.tensor([0, 1, 2, 3, 4, 5], dtype=torch.int64).npu()
        output = torch.empty([9], dtype=torch.int64).npu()
        input_split_sizes = [2, 2, 1, 1]
        output_split_sizes = [2, 3, 2, 2]
    elif rank == 1:
        input = torch.tensor([10, 11, 12, 13, 14, 15, 16, 17, 18], dtype=torch.int64).npu()
        output = torch.empty([7], dtype=torch.int64).npu()
        input_split_sizes = [3, 2, 2, 2]
        output_split_sizes = [2, 2, 1, 2]
    elif rank == 2:
        input = torch.tensor([20, 21, 22, 23, 24], dtype=torch.int64).npu()
        output = torch.empty([6], dtype=torch.int64).npu()
        input_split_sizes = [2, 1, 1, 1]  
        output_split_sizes = [1, 2, 1, 2]
    elif rank == 3:
        input = torch.tensor([30, 31, 32, 33, 34, 35, 36], dtype=torch.int64).npu()
        output = torch.empty([5], dtype=torch.int64).npu()
        input_split_sizes = [2, 2, 2, 1]
        output_split_sizes = [1, 2, 1, 1]

    print("input: ", input)

    model = All2allsinge_split_size().npu()

    from torchair.core.utils import logger
    import logging
    from torchair.configs.compiler_config import CompilerConfig
    config = CompilerConfig()
    # logger.setLevel(logging.DEBUG)
    config.debug.graph_dump.type = "pbtxt"
    # config.experimental_config.static_model_ops_lower_limit = 4
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    # dist.all_to_all_single(output, input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1])
    model = torch.compile(model, backend=npu_backend, dynamic=False, fullgraph=True)
    # dist.all_to_all_single(output, input, output_split_sizes=[3, 1], input_split_sizes=[1, 3])
    with torch.no_grad():
        output = model(input, output, input_split_sizes, output_split_sizes)

    # output = all_to_all_single(input, output_split_sizes=[2, 2], input_split_sizes=[2, 2], group=distributed_c10d._get_default_group()) # 会报错。也不提示是size给错了，这个有问题
    # output = all_to_all_single(input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1], group=distributed_c10d._get_default_group())
    # output = funcol.all_to_all_single(input, output_split_sizes=[1, 3], input_split_sizes=[1, 3], group=distributed_c10d._get_default_group())
    print("output: ", output)
    dist.destroy_process_group()

def test_alltoall_single_static(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input = torch.arange(4) + rank * 4
    input = input.npu()
    print("input: ", input)
    output = output = torch.empty([4], dtype=torch.int64).npu()

    model = All2allsinge().npu()

    from torchair.core.utils import logger
    import logging
    from torchair.configs.compiler_config import CompilerConfig
    config = CompilerConfig()
    # logger.setLevel(logging.DEBUG)
    config.debug.graph_dump.type = "pbtxt"
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    model = torch.compile(model, backend=npu_backend, dynamic=False, fullgraph=True)
    # dist.all_to_all_single(output, input, output_split_sizes=[3, 1], input_split_sizes=[1, 3])
    with torch.no_grad():
        output = model(input, output)
    dist.all_to_all_single(output, input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1])
    # output = all_to_all_single(input, output_split_sizes=[2, 2], input_split_sizes=[2, 2], group=distributed_c10d._get_default_group()) # 会报错。也不提示是size给错了，这个有问题
    # output = all_to_all_single(input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1], group=distributed_c10d._get_default_group())
    # output = funcol.all_to_all_single(input, output_split_sizes=[1, 3], input_split_sizes=[1, 3], group=distributed_c10d._get_default_group())
    print("output: ", output)
    dist.destroy_process_group()

class All2allsingeNoSplit(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, input, output):
        dist.all_to_all_single(output, input)
        return output

def test_alltoall_single_NoSplit(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input = torch.arange(4) + rank * 4
    input = input.npu()
    print("input: ", input)
    output = output = torch.empty([4], dtype=torch.int64).npu()

    model = All2allsingeNoSplit().npu()

    from torchair.core.utils import logger
    import logging
    from torchair.configs.compiler_config import CompilerConfig
    config = CompilerConfig()
    # logger.setLevel(logging.DEBUG)
    config.debug.graph_dump.type = "pbtxt"
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    # dist.all_to_all_single(output, input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1])
    model = torch.compile(model, backend=npu_backend, dynamic=True, fullgraph=True)
    # dist.all_to_all_single(output, input, output_split_sizes=[3, 1], input_split_sizes=[1, 3])
    with torch.no_grad():
        output = model(input, output)

    # output = all_to_all_single(input, output_split_sizes=[2, 2], input_split_sizes=[2, 2], group=distributed_c10d._get_default_group()) # 会报错。也不提示是size给错了，这个有问题
    # output = all_to_all_single(input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1], group=distributed_c10d._get_default_group())
    # output = funcol.all_to_all_single(input, output_split_sizes=[1, 3], input_split_sizes=[1, 3], group=distributed_c10d._get_default_group())
    print("output: ", output)
    dist.destroy_process_group()


def test_alltoall_single_NoSplit_static(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input = torch.arange(4) + rank * 4
    input = input.npu()
    print("input: ", input)
    output = output = torch.empty([4], dtype=torch.int64).npu()

    model = All2allsingeNoSplit().npu()

    from torchair.core.utils import logger
    import logging
    from torchair.configs.compiler_config import CompilerConfig
    config = CompilerConfig()
    # logger.setLevel(logging.DEBUG)
    config.debug.graph_dump.type = "pbtxt"
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    dist.all_to_all_single(output, input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1])
    model = torch.compile(model, backend=npu_backend, dynamic=False, fullgraph=True)
    # dist.all_to_all_single(output, input, output_split_sizes=[3, 1], input_split_sizes=[1, 3])
    with torch.no_grad():
        output = model(input, output)

    # output = all_to_all_single(input, output_split_sizes=[2, 2], input_split_sizes=[2, 2], group=distributed_c10d._get_default_group()) # 会报错。也不提示是size给错了，这个有问题
    # output = all_to_all_single(input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1], group=distributed_c10d._get_default_group())
    # output = funcol.all_to_all_single(input, output_split_sizes=[1, 3], input_split_sizes=[1, 3], group=distributed_c10d._get_default_group())
    print("output: ", output)
    dist.destroy_process_group()


class All2allsingeNoSplitInOutput(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, input, output):
        input = input + 1
        dist.all_to_all_single(output, input)
        return output + 1

def test_alltoall_single_NoSplit_static_inoutput(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input = torch.arange(4) + rank * 4
    input = input.npu()
    print("input: ", input)
    output = output = torch.empty([4], dtype=torch.int64).npu()

    model = All2allsingeNoSplitInOutput().npu()

    from torchair.core.utils import logger
    import logging
    from torchair.configs.compiler_config import CompilerConfig
    config = CompilerConfig()
    # logger.setLevel(logging.DEBUG)
    config.debug.graph_dump.type = "pbtxt"
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    dist.all_to_all_single(output, input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1])
    model = torch.compile(model, backend=npu_backend, dynamic=False, fullgraph=True)
    # dist.all_to_all_single(output, input, output_split_sizes=[3, 1], input_split_sizes=[1, 3])
    with torch.no_grad():
        output = model(input, output)

    # output = all_to_all_single(input, output_split_sizes=[2, 2], input_split_sizes=[2, 2], group=distributed_c10d._get_default_group()) # 会报错。也不提示是size给错了，这个有问题
    # output = all_to_all_single(input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1], group=distributed_c10d._get_default_group())
    # output = funcol.all_to_all_single(input, output_split_sizes=[1, 3], input_split_sizes=[1, 3], group=distributed_c10d._get_default_group())
    print("output: ", output)
    dist.destroy_process_group()

def test_alltoall_single_export(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input = torch.arange(4) + rank * 4
    input = input.npu()
    print("input: ", input)
    output = output = torch.empty([4], dtype=torch.int64).npu()

    model = All2allsingeNoSplitInOutput().npu()

    from torchair.core.utils import logger
    import logging
    from torchair.configs.compiler_config import CompilerConfig
    config = CompilerConfig()
    # logger.setLevel(logging.DEBUG)
    config.debug.graph_dump.type = "pbtxt"
    npu_backend = torchair.get_npu_backend(compiler_config=config)

    torchair.dynamo_export(input, output, model=model, dynamic=True)
    # output = all_to_all_single(input, output_split_sizes=[2, 2], input_split_sizes=[2, 2], group=distributed_c10d._get_default_group()) # 会报错。也不提示是size给错了，这个有问题
    # output = all_to_all_single(input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1], group=distributed_c10d._get_default_group())
    # output = funcol.all_to_all_single(input, output_split_sizes=[1, 3], input_split_sizes=[1, 3], group=distributed_c10d._get_default_group())
    print("output: ", output)
    dist.destroy_process_group()

class All2all(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, input, output):
        dist.all_to_all(output, input)
        return output

def test_alltoall(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input = torch.arange(4) + rank * 4
    input = input.npu()
    input = list(input.chunk(4))

    print("input: ", input)
    output = torch.empty([4], dtype=torch.int64).npu()
    output = list(output.chunk(4))

    model = All2all().npu()
    from torchair.core.utils import logger
    import logging
    from torchair.configs.compiler_config import CompilerConfig
    config = CompilerConfig()
    # logger.setLevel(logging.DEBUG)
    # config.experimental_config.static_model_ops_lower_limit = 4
    config.debug.graph_dump.type = "pbtxt"
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    model = torch.compile(model, backend=npu_backend, dynamic=True, fullgraph=True)
    with torch.no_grad():
        output = model(input, output)
    # dist.all_to_all(output, input)
    # dist.all_to_all_single(output, input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1])
    # output = all_to_all_single(input, output_split_sizes=[2, 2], input_split_sizes=[2, 2], group=distributed_c10d._get_default_group()) # 会报错。也不提示是size给错了，这个有问题
    # output = all_to_all_single(input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1], group=distributed_c10d._get_default_group())
    # output = funcol.all_to_all_single(input, output_split_sizes=[1, 3], input_split_sizes=[1, 3], group=distributed_c10d._get_default_group())

    print("output: ", output)
    dist.destroy_process_group()

def test_alltoall2(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input = torch.arange(4) + rank * 4
    input = input.npu()
    input = list(input.chunk(4))

    print("input: ", input)
    output = torch.empty([4], dtype=torch.int64).npu()
    output = list(output.chunk(4))

    model = All2all().npu()
    from torchair.core.utils import logger
    import logging
    from torchair.configs.compiler_config import CompilerConfig
    config = CompilerConfig()
    # logger.setLevel(logging.DEBUG)
    # config.experimental_config.static_model_ops_lower_limit = 4
    config.debug.graph_dump.type = "pbtxt"
    output = model(input, output)
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    model = torch.compile(model, backend=npu_backend, dynamic=False, fullgraph=True)
    with torch.no_grad():
        output = model(input, output)
    # dist.all_to_all(output, input)
    # dist.all_to_all_single(output, input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1])
    # output = all_to_all_single(input, output_split_sizes=[2, 2], input_split_sizes=[2, 2], group=distributed_c10d._get_default_group()) # 会报错。也不提示是size给错了，这个有问题
    # output = all_to_all_single(input, output_split_sizes=[1,1,1,1], input_split_sizes=[1,1,1,1], group=distributed_c10d._get_default_group())
    # output = funcol.all_to_all_single(input, output_split_sizes=[1, 3], input_split_sizes=[1, 3], group=distributed_c10d._get_default_group())

    print("output: ", output)
    dist.destroy_process_group()

def test_alltoall3(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input = torch.arange(4) + rank * 4
    input = input.npu()
    input = list(input.chunk(4))

    print("input: ", input)
    output = torch.empty([4], dtype=torch.int64).npu()
    output = list(output.chunk(4))

    model = All2all().npu()
    torchair.dynamo_export(input, output, model=model, dynamic=True)

    print("output: ", output)
    dist.destroy_process_group()

def mp():
    world_size = 4
    # =================  case 1 基本入图场景 动态图 + 单算子混跑 + split_sizes入参==================
    torch.multiprocessing.spawn(test_alltoall_single_dynamic, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 1 pass =============================")
    # =================  case 2 基本入图场景 静态图 + 单算子混跑 + split_sizes入参==================
    torch.multiprocessing.spawn(test_alltoall_single_static, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 2 pass =============================")
    # =================  case 3 基本入图场景 动态图 + 单算子混跑 + 无split_sizes入参==================
    torch.multiprocessing.spawn(test_alltoall_single_NoSplit, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 3 pass =============================")
    # =================  case 4 基本入图场景 静态图 + 单算子混跑 + 无split_sizes入参==================
    torch.multiprocessing.spawn(test_alltoall_single_NoSplit_static, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 4 pass =============================")
    # =================  case 5 基本入图场景 静态图 + 单算子混跑 + 无split_sizes入参，不直连输入输入输出==================
    torch.multiprocessing.spawn(test_alltoall_single_NoSplit_static_inoutput, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 5 pass =============================")
    # =================  case 6 基本入图场景 静态图 + 单算子混跑 + 无split_sizes入参，不直连输入输入输出 export==================
    torch.multiprocessing.spawn(test_alltoall_single_export, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 6 pass =============================")
    # =================  case 7 动态图 + split_sizes入参不等分==================
    torch.multiprocessing.spawn(test_alltoall_single_dynamic_split_size, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 7 pass =============================")
    # =================  case 8 静态图 + split_sizes入参不等分==================
    torch.multiprocessing.spawn(test_alltoall_single_static_split_size, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 8 pass =============================")
    # =================  case 9 动态图 + all2all基本用例==================
    torch.multiprocessing.spawn(test_alltoall, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 9 pass =============================")
    # =================  case 10 动态图 + all2all 单算子图混跑==================
    torch.multiprocessing.spawn(test_alltoall2, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 10 pass =============================")

    # =================  case 10 动态图 + all2all tensor不等分(原生api暂不支持不等分场景)==================
    print("==================case 10 pass =============================")

    # =================  case 11 动态图 + all2all export==================
    torch.multiprocessing.spawn(test_alltoall3, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 11 pass =============================")
    

if __name__ == '__main__':
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "29506"
    mp()
