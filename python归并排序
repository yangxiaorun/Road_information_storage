import os
from typing import Any, Dict, Iterator, List, Optional, Tuple, Union
import torch
import torch.distributed._functional_collectives as funcol
import torch.distributed as dist
from torch.distributed import distributed_c10d
import torch.distributed.distributed_c10d as c10d
import torch_npu
import torchair
import torchair.ge_concrete_graph.ge_converter.experimental.hcom_alltoall
from torchair.core.utils import logger
import logging
from torchair.configs.compiler_config import CompilerConfig

class All2allsinge(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, input1, output1):
        dist.all_to_all_single(output1, input1, output_split_sizes=[1, 1, 1, 1], input_split_sizes=[1, 1, 1, 1])
        return output1


def test_alltoall_single_dynamic(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    tensor_input = torch.arange(4) + rank * 4
    tensor_input = tensor_input.npu()
    print("input: ", tensor_input)
    tensor_output = torch.empty([4], dtype=torch.int64).npu()

    model = All2allsinge().npu()
    config = CompilerConfig()
    config.debug.graph_dump.type = "pbtxt"
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    dist.all_to_all_single(tensor_output, tensor_input, output_split_sizes=[1, 1, 1, 1], input_split_sizes=[1, 1, 1, 1])
    model = torch.compile(model, backend=npu_backend, dynamic=True, fullgraph=True)
    with torch.no_grad():
        tensor_output = model(tensor_input, tensor_output)
    print("output: ", tensor_output)
    dist.destroy_process_group()


class all2allsinge_split_size(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, input1, output1, input_split_sizes, output_split_sizes):
        dist.all_to_all_single(output1, input1, output_split_sizes=output_split_sizes,
                               input_split_sizes=input_split_sizes)
        return output1


def test_alltoall_single_dynamic_split_size(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    if rank == 0:
        input1 = torch.tensor([0, 1, 2, 3, 4, 5], dtype=torch.int64).npu()
        output1 = torch.empty([9], dtype=torch.int64).npu()
        input_split_sizes = [2, 2, 1, 1]
        output_split_sizes = [2, 3, 2, 2]
    elif rank == 1:
        input1 = torch.tensor([10, 11, 12, 13, 14, 15, 16, 17, 18], dtype=torch.int64).npu()
        output1 = torch.empty([7], dtype=torch.int64).npu()
        input_split_sizes = [3, 2, 2, 2]
        output_split_sizes = [2, 2, 1, 2]
    elif rank == 2:
        input1 = torch.tensor([20, 21, 22, 23, 24], dtype=torch.int64).npu()
        output1 = torch.empty([6], dtype=torch.int64).npu()
        input_split_sizes = [2, 1, 1, 1]  
        output_split_sizes = [1, 2, 1, 2]
    elif rank == 3:
        input1 = torch.tensor([30, 31, 32, 33, 34, 35, 36], dtype=torch.int64).npu()
        output1 = torch.empty([5], dtype=torch.int64).npu()
        input_split_sizes = [2, 2, 2, 1]
        output_split_sizes = [1, 2, 1, 1]

    print("input: ", input1)

    model = all2allsinge_split_size().npu()

    config = CompilerConfig()
    config.debug.graph_dump.type = "pbtxt"
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    model = torch.compile(model, backend=npu_backend, dynamic=True, fullgraph=True)
    with torch.no_grad():
        output1 = model(input1, output1, input_split_sizes, output_split_sizes)

    print("output: ", output1)
    dist.destroy_process_group()


def test_alltoall_single_static_split_size(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    if rank == 0:
        input = torch.tensor([0, 1, 2, 3, 4, 5], dtype=torch.int64).npu()
        output = torch.empty([9], dtype=torch.int64).npu()
        input_split_sizes = [2, 2, 1, 1]
        output_split_sizes = [2, 3, 2, 2]
    elif rank == 1:
        input = torch.tensor([10, 11, 12, 13, 14, 15, 16, 17, 18], dtype=torch.int64).npu()
        output = torch.empty([7], dtype=torch.int64).npu()
        input_split_sizes = [3, 2, 2, 2]
        output_split_sizes = [2, 2, 1, 2]
    elif rank == 2:
        input = torch.tensor([20, 21, 22, 23, 24], dtype=torch.int64).npu()
        output = torch.empty([6], dtype=torch.int64).npu()
        input_split_sizes = [2, 1, 1, 1]  
        output_split_sizes = [1, 2, 1, 2]
    elif rank == 3:
        input = torch.tensor([30, 31, 32, 33, 34, 35, 36], dtype=torch.int64).npu()
        output = torch.empty([5], dtype=torch.int64).npu()
        input_split_sizes = [2, 2, 2, 1]
        output_split_sizes = [1, 2, 1, 1]

    print("input: ", input)

    model = all2allsinge_split_size().npu()

    config = CompilerConfig()
    config.debug.graph_dump.type = "pbtxt"
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    model = torch.compile(model, backend=npu_backend, dynamic=False, fullgraph=True)
    with torch.no_grad():
        output = model(input, output, input_split_sizes, output_split_sizes)
    print("output: ", output)
    dist.destroy_process_group()


def test_alltoall_single_static(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input1 = torch.arange(4) + rank * 4
    input1 = input1.npu()
    print("input: ", input1)
    output1 = torch.empty([4], dtype=torch.int64).npu()

    model = All2allsinge().npu()

    config = CompilerConfig()
    config.debug.graph_dump.type = "pbtxt"
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    model = torch.compile(model, backend=npu_backend, dynamic=False, fullgraph=True)
    with torch.no_grad():
        output1 = model(input1, output1)
    dist.all_to_all_single(output1, input1, output_split_sizes=[1, 1, 1, 1],
                           input_split_sizes=[1, 1, 1, 1])
    print("output: ", output1)
    dist.destroy_process_group()


class all2all_singe_no_split(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, input, output):
        dist.all_to_all_single(output, input)
        return output


def test_alltoall_single_NoSplit(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input1 = torch.arange(4) + rank * 4
    input1 = input1.npu()
    print("input: ", input)
    output1 = torch.empty([4], dtype=torch.int64).npu()

    model = all2all_singe_no_split().npu()
    config = CompilerConfig()
    config.debug.graph_dump.type = "pbtxt"
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    model = torch.compile(model, backend=npu_backend, dynamic=True, fullgraph=True)
    with torch.no_grad():
        output1 = model(input1, output1)
    print("output: ", output1)
    dist.destroy_process_group()


def test_alltoall_single_NoSplit_static(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input1 = torch.arange(4) + rank * 4
    input1 = input1.npu()
    print("input: ", input1)
    output1 = torch.empty([4], dtype=torch.int64).npu()

    model = all2all_singe_no_split().npu()

    config = CompilerConfig()
    config.debug.graph_dump.type = "pbtxt"
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    dist.all_to_all_single(output1, input1, output_split_sizes=[1, 1, 1, 1], input_split_sizes=[1, 1, 1, 1])
    model = torch.compile(model, backend=npu_backend, dynamic=False, fullgraph=True)
    with torch.no_grad():
        output1 = model(input1, output1)
    print("output: ", output1)
    dist.destroy_process_group()


class all2all_singe_no_split_inoutput(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, input1, output1):
        input1 = input1 + 1
        dist.all_to_all_single(output1, input1)
        return output1 + 1


def test_alltoall_single_NoSplit_static_inoutput(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input1 = torch.arange(4) + rank * 4
    input1 = input1.npu()
    print("input: ", input1)
    output1 = torch.empty([4], dtype=torch.int64).npu()

    model = all2all_singe_no_split_inoutput().npu()

    config = CompilerConfig()
    config.debug.graph_dump.type = "pbtxt"
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    dist.all_to_all_single(output1, input1, output_split_sizes=[1, 1, 1, 1], input_split_sizes=[1, 1, 1, 1])
    model = torch.compile(model, backend=npu_backend, dynamic=False, fullgraph=True)
    with torch.no_grad():
        output = model(input1, output1)
    print("output: ", output1)
    dist.destroy_process_group()


def test_alltoall_single_export(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input1 = torch.arange(4) + rank * 4
    input1 = input1.npu()
    print("input: ", input1)
    output1 = torch.empty([4], dtype=torch.int64).npu()

    model = all2all_singe_no_split_inoutput().npu()
    config = CompilerConfig()
    config.debug.graph_dump.type = "pbtxt"
    npu_backend = torchair.get_npu_backend(compiler_config=config)

    torchair.dynamo_export(input1, output1, model=model, dynamic=True)
    dist.destroy_process_group()


class all2all(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, input1, output1):
        dist.all_to_all(output1, input1)
        return output1


def test_alltoall(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input1 = torch.arange(4) + rank * 4
    input1 = input1.npu()
    input1 = list(input1.chunk(4))

    print("input: ", input1)
    output1 = torch.empty([4], dtype=torch.int64).npu()
    output1 = list(output1.chunk(4))

    model = all2all().npu()
    config = CompilerConfig()
    config.debug.graph_dump.type = "pbtxt"
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    model = torch.compile(model, backend=npu_backend, dynamic=True, fullgraph=True)
    with torch.no_grad():
        output1 = model(input1, output1)
    print("out output1: ", output1)
    dist.destroy_process_group()


def test_alltoall2(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input1 = torch.arange(4) + rank * 4
    input1 = input1.npu()
    input1 = list(input1.chunk(4))

    print("input: ", input1)
    output1 = torch.empty([4], dtype=torch.int64).npu()
    output1 = list(output1.chunk(4))

    model = all2all().npu()
    config = CompilerConfig()
    config.debug.graph_dump.type = "pbtxt"
    output1 = model(input1, output1)
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    model = torch.compile(model, backend=npu_backend, dynamic=False, fullgraph=True)
    with torch.no_grad():
        output1 = model(input1, output1)
    print("output: ", output1)
    dist.destroy_process_group()


def test_alltoall3(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input1 = torch.arange(4) + rank * 4
    input1 = input1.npu()
    input1 = list(input1.chunk(4))

    print("input: ", input1)
    output1 = torch.empty([4], dtype=torch.int64).npu()
    output1 = list(output1.chunk(4))

    model = all2all().npu()
    torchair.dynamo_export(input1, output1, model=model, dynamic=True)
    dist.destroy_process_group()

def test_alltoall4(rank, world_size):
    torch.npu.set_device(rank)
    dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)
    input_list = [(torch.zeros(rank + 1, 1) + rank).float().npu() for i in range(world_size)]
    output_list = [torch.empty(i + 1, 1).float().npu() for i in range(world_size)]
    print("input_list ", input_list)
    model = all2all().npu()
    config = CompilerConfig()
    config.debug.graph_dump.type = "pbtxt"
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    model = torch.compile(model, backend=npu_backend, dynamic=False, fullgraph=True)
    with torch.no_grad():
        output_list = model(input_list, output_list)
    print("out output_list: ", output_list)
    dist.destroy_process_group()

def mp():
    world_size = 4
    # =================  case 1 基本入图场景 动态图 + 单算子混跑 + split_sizes入参==================
    torch.multiprocessing.spawn(test_alltoall_single_dynamic, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 1 pass =============================")
    # =================  case 2 基本入图场景 静态图 + 单算子混跑 + split_sizes入参==================
    torch.multiprocessing.spawn(test_alltoall_single_static, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 2 pass =============================")
    # =================  case 3 基本入图场景 动态图 + 单算子混跑 + 无split_sizes入参==================
    torch.multiprocessing.spawn(test_alltoall_single_NoSplit, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 3 pass =============================")
    # =================  case 4 基本入图场景 静态图 + 单算子混跑 + 无split_sizes入参==================
    torch.multiprocessing.spawn(test_alltoall_single_NoSplit_static, args=(world_size, ),
                                nprocs=world_size, join=True)
    print("==================case 4 pass =============================")
    # =================  case 5 基本入图场景 静态图 + 单算子混跑 + 无split_sizes入参，不直连输入输入输出============
    torch.multiprocessing.spawn(test_alltoall_single_NoSplit_static_inoutput,
                                args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 5 pass =============================")
    # =================  case 6 基本入图场景 静态图 + 单算子混跑 + 无split_sizes入参，不直连输入输入输出 export======
    torch.multiprocessing.spawn(test_alltoall_single_export, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 6 pass =============================")
    # =================  case 7 动态图 + split_sizes入参不等分==================
    torch.multiprocessing.spawn(test_alltoall_single_dynamic_split_size, args=(world_size, ),
                                nprocs=world_size, join=True)
    print("==================case 7 pass =============================")
    # =================  case 8 静态图 + split_sizes入参不等分==================
    torch.multiprocessing.spawn(test_alltoall_single_static_split_size, args=(world_size, ),
                                nprocs=world_size, join=True)
    print("==================case 8 pass =============================")
    # =================  case 9 动态图 + all2all基本用例==================
    torch.multiprocessing.spawn(test_alltoall, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 9 pass =============================")
    # =================  case 10 动态图 + all2all 单算子图混跑==================
    torch.multiprocessing.spawn(test_alltoall2, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 10 pass =============================")

    # =================  case 11 动态图 + all2all tensor不等分==================
    torch.multiprocessing.spawn(test_alltoall4, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 11 pass =============================")

    # =================  case 12 动态图 + all2all export==================
    torch.multiprocessing.spawn(test_alltoall3, args=(world_size, ), nprocs=world_size, join=True)
    print("==================case 12 pass =============================")
    

if __name__ == '__main__':
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "29506"
    mp()
